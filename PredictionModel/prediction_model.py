# -*- coding: utf-8 -*-
"""CryptoPredict.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LHuZK0S-AWaGC7ijgOQFwlF3FAKSUg1W
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import keras
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Bidirectional, LSTM, Dropout, Dense, Activation
from tensorflow.compat.v1.keras.layers import Bidirectional, CuDNNLSTM, Dropout, Dense, Activation
from matplotlib.dates import DateFormatter, AutoDateLocator
import lightgbm as lgb

def plot_series(x, y, format="-", start=0, end=None,
                title=None, xlabel=None, ylabel=None, legend=None ):
    """
    Visualizes time series data

    Args:
      x (array of int) - contains values for the x-axis
      y (array of int or tuple of arrays) - contains the values for the y-axis
      format (string) - line style when plotting the graph
      start (int) - first time step to plot
      end (int) - last time step to plot
      title (string) - title of the plot
      xlabel (string) - label for the x-axis
      ylabel (string) - label for the y-axis
      legend (list of strings) - legend for the plot
    """

    # Setup dimensions of the graph figure
    plt.figure(figsize=(10, 6))

    # Check if there are more than two series to plot
    if type(y) is tuple:

      # Loop over the y elements
      for y_curr in y:

        # Plot the x and current y values
        plt.plot(x[start:end], y_curr[start:end], format)

    else:
      # Plot the x and y values
      plt.plot(x[start:end], y[start:end], format)

    # Label the x-axis
    plt.xlabel(xlabel)

    # Label the y-axis
    plt.ylabel(ylabel)

    # Set the legend
    if legend:
      plt.legend(legend)

    # Set the title
    plt.title(title)

    # Overlay a grid on the graph
    plt.grid(True)

    # Draw the graph on screen
    plt.show()

def get_data(file_name):
  df = pd.read_csv(file_name)
  df = df[::-1].reset_index(drop=True)
  df['Price'] = df['Price'].str.replace(',', '').astype(float)
  return df

def split_data_set(df, ratio):
  split_point = int((len(df)) * ratio)
  print(split_point)
  time_train = df['Date'][:split_point]
  x_train = df['Price'][:split_point]

  time_valid = df['Date'][split_point:]
  x_valid = df['Price'][split_point:]

  return time_train, x_train, time_valid, x_valid

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    """Generates dataset windows

    Args:
      series (array of float) - contains the values of the time series
      window_size (int) - the number of time steps to include in the feature
      batch_size (int) - the batch size
      shuffle_buffer(int) - buffer size to use for the shuffle method

    Returns:
      dataset (TF Dataset) - TF Dataset containing time windows
    """

    # Generate a TF Dataset from the series values
    dataset = tf.data.Dataset.from_tensor_slices(series)

    # Window the data but only take those with the specified size
    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)

    # Flatten the windows by putting its elements in a single batch
    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))

    # Create tuples with features and labels
    dataset = dataset.map(lambda window: (window[:-1], window[-1]))

    # Shuffle the windows
    dataset = dataset.shuffle(shuffle_buffer)

    # Create batches of windows
    dataset = dataset.batch(batch_size).prefetch(1)

    return dataset

def add_features(df):
  window = 10
  df['Price_MA'] = df['Price'].rolling(window=window_size).mean()
  df['Price_1Lag'] = df['Price'].shift(1)
  df['Price_2Lag'] = df['Price'].shift(2)
  # df.fillna(0, inplace=True)
  df.dropna(inplace=True)
  return df

# Parameters

window_size = 3
batch_size = 32
shuffle_buffer_size = 1000

def get_train_set(x_train):

  # Generate the dataset windows
  # train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)
  train_set = add_features(train_set)
  return train_set

# Build the Model

def build_model(window_size):
  model = tf.keras.models.Sequential([
  tf.keras.layers.Conv1D(filters=64, kernel_size=3,
                      strides=1,
                      activation="relu",
                      padding='causal',
                      input_shape=[window_size, 1]),
  tf.keras.layers.LSTM(64, return_sequences=True),
  tf.keras.layers.LSTM(64),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x * 400)
])

 # Print the model summary
  model.summary()
  return model

def build_model_lgbm():
  params = {
    'objective': 'regression',
    'metric': 'rmse',  # Root Mean Squared Error
    'boosting_type': 'gbdt',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.9
  }
  lgb_model = lgb.LGBMRegressor(**params, n_estimators=500)
  return lgb_model

def tune_model(model, train_set):
  init_weights = model.get_weights()
  lr_schedule = tf.keras.callbacks.LearningRateScheduler(
    lambda epoch: 1e-8 * 10**(epoch / 20))

  # Initialize the optimizer
  optimizer = tf.keras.optimizers.SGD(momentum=0.9)

  # Set the training parameters
  model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer)

  # Train the model
  history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])

  # Define the learning rate array
  lrs = 1e-8 * (10 ** (np.arange(100) / 20))

  # Set the figure size
  plt.figure(figsize=(10, 6))

  # Set the grid
  plt.grid(True)

  # Plot the loss in log scale
  plt.semilogx(lrs, history.history["loss"])

  # Increase the tickmarks size
  plt.tick_params('both', length=10, width=1, which='both')

  # Set the plot boundaries
  plt.axis([1e-8, 1e-3, 0, 100])

  # Reset states generated by Keras
  tf.keras.backend.clear_session()

  # Reset the weights
  model.set_weights(init_weights)

  # Set the learning rate
  learning_rate = 8e-7

  # Set the optimizer
  optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)

  # Set the training parameters
  model.compile(loss=tf.keras.losses.Huber(),
                optimizer=optimizer,
                metrics=["mae"])
  return model

# Train the model

def run_model(model, train_set):
  history = model.fit(train_set,epochs=100)

  # Get mae and loss from history log
  mae=history.history['mae']
  loss=history.history['loss']

  # Get number of epochs
  epochs=range(len(loss))

  # Plot mae and loss
  plot_series(
      x=epochs,
      y=(mae, loss),
      title='MAE and Loss',
      xlabel='MAE',
      ylabel='Loss',
      legend=['MAE', 'Loss']
      )

  # Only plot the last 80% of the epochs
  zoom_split = int(epochs[-1] * 0.2)
  epochs_zoom = epochs[zoom_split:]
  mae_zoom = mae[zoom_split:]
  loss_zoom = loss[zoom_split:]

  # Plot zoomed mae and loss
  plot_series(
      x=epochs_zoom,
      y=(mae_zoom, loss_zoom),
      title='MAE and Loss',
      xlabel='MAE',
      ylabel='Loss',
      legend=['MAE', 'Loss']
      )

def model_forecast(model, series, window_size, batch_size):
    """Uses an input model to generate predictions on data windows

    Args:
      model (TF Keras Model) - model that accepts data windows
      series (array of float) - contains the values of the time series
      window_size (int) - the number of time steps to include in the window
      batch_size (int) - the batch size

    Returns:
      forecast (numpy array) - array containing predictions
    """

    # Generate a TF Dataset from the series values
    dataset = tf.data.Dataset.from_tensor_slices(series)

    # Window the data but only take those with the specified size
    dataset = dataset.window(window_size, shift=1, drop_remainder=True)

    # Flatten the windows by putting its elements in a single batch
    dataset = dataset.flat_map(lambda w: w.batch(window_size))

    # Create batches of windows
    dataset = dataset.batch(batch_size).prefetch(1)

    # Get predictions on the entire dataset
    forecast = model.predict(dataset)

    return forecast

def model_forecast_runner(df, ratio, time_valid, x_valid):
  split_point = int((len(df)) * ratio)
  series = df['Price']
  forecast_series = series[split_point-window_size:-1]

  forecast = model_forecast(model, forecast_series, window_size, batch_size)
  # Drop single dimensional axis
  results = forecast.squeeze()
  print(results)

  # Plot the results
  plot_series(time_valid, (x_valid, results))
  print(tf.keras.metrics.mean_absolute_error(x_valid, results).numpy())

"""### Functions LGBM Model"""

def train_test_split(df):
  split_point = (int(len(df) * 0.75))
  df_new = df.drop(['Date', 'Open', 'High', 'Low', 'Vol.', 'Change %'], axis=1)
  df_train = df_new[:split_point]
  df_test = df_new[split_point:]
  x_train = df_train.drop('Price', axis=1)
  y_train = df_train['Price']
  x_test = df_test.drop('Price', axis=1)
  y_test = df_test['Price']
  return x_train, y_train, x_test, y_test

"""# Testing the Model"""

df = get_data("Ethereum Historical Data - Investing.com India.csv")

plot_series(x=df.index, y=df['Price'], start=0)

df

time_train, x_train, time_valid, x_valid = split_data_set(df, 0.75)

train_set = get_train_set(x_train)

model = build_model(30)

model = tune_model(model, train_set)

run_model(model, train_set)

model_forecast_runner(df, 0.75, time_valid, x_valid)



"""### LGBM Model test Run"""

model = build_model_lgbm()

df_added = add_features(df)

df_added

x_train, y_train, x_test, y_test = train_test_split(df_added)

x_train

model.fit(x_train, y_train)

y_pred = model.predict(x_test)

plt.plot(y_test.index, y_test)
plt.plot(y_test.index, y_pred)
plt.show()

